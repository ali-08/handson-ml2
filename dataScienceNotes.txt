3 types of gradient descent:-
1) Batch Gradient Descent: Parameters get updated after the entire examples are processed; also known as epoch gradient descent
2) Stochastic Gradient Descent: Parameters get updated after each example is processed
3) mini batch Gradient Descent: Paramenters get updated after each mini batch is processed; combo of Batch and Stochastic

data snooping bias: overfitting

Norms:
The L1 norm that is calculated as the sum of the absolute values of the vector.
||v||1 = |a1| + |a2| + |a3|
[1 2 3]
 
6.0

The L2 norm that is calculated as the square root of the sum of the squared vector values.
||v||2 = sqrt(a1^2 + a2^2 + a3^2)

[1 2 3]
 
3.74165738677

The max norm that is calculated as the maximum vector values.
 	
[1 2 3]
 
3.0
--------------------------------------------------------------------------------------------------------------------------------------------------------------------------
Model Parameter: configuration varaible, internal to model, whose value can be estimated from data
Examples:
1) weights in artificial neural network
2) support vectors in SVM
3) theta in Linear and Logistic Regression

Model Hypermeter: configuration external to model whose value cannot be estimated from data
Examples:
1) learing rate for training a neural network
2) C and sigma in support vector Machine
3) k in KNNs

If you have to specify a model parameter manually than it's probably a model hyperparameter
--------------------------------------------------------------------------------------------------------------------------------------------------------------------------



Chp #1:
prediction = inference

CHAPTER # 2:

Stratified Sampling: take examples from each startum in the test set so that actual representation from population is maintained. 49% men and 51% women in society.

pd.cut: used when data values need to be transformed from continuous to categorical

Permanent effects:
-) housing.drop('attribute', axis=1, inplace=True)
-) housing = housing.drop('attribute', axis=1)

Temporary effects:
-) housing.drop('attribute', axis=1) # Without keyword inplace
-) housing.drop('attribute', axis=1) # Without =

iloc: till particular index
loc: till particular value in index



Data Cleaning:-
1) getting rid of rows: pd.dropna(subset=["list of attributes"])
2) removing columns: pd.drop("col_name", axis=1)
3) replacing with median: using sklearn.preprocessing's imputer



Handling Categorical attributes thru one-hot encoding
If you have large number of categories then one-hot encoding will result in large number of features, slowing down performance of the model.
Soln:
-) replace cateorical with numerical features
-) replace each category with learnable low dimension vector called embedding. Each category is learned during training. See chp#13

Use custom transformers for extra attribs[hyperparameter]



Feature Scaling:-
Min-max Scaling(Normalization): range 0-1, MinMaxScaler is Sklearn's transformer for it, change feature_range hypermeter if u don't wanna range of 0-1
Standardization: subtract mean then divide by sd. range not in 0-1, adv: less affected by outliers, StandardScaler is sklearn's transformer for it
REMEMBER: fit the scalers to the training data only, not to the full dataset. Then use it to transform the full dataset



Transformation Pipelines:
Purpose: to help with sequence of transformations
Parameter: list of (name, transform) tuples (implementing fit/transform) that are chained



GridSearchCV:-
We tell it which hyperparameters we wanna play with, and what values we wanna try out, and it will try out all possible values using cross-validation
Each combination is given in a dictionary.
It acts as a model as it has a fit method.

grid_search.best_estimator_ :- returns final model
grid_search.best_params_ :- returns the best parameters


RandomizedSearhCV:-
Used instead of GridSearchCV when hyperparameter search space is large
It evaluates a given number of random combinations by selecting a random value of each hyperparameter at every iteration


-----------------------------------------------*************************************************************************************---------------------------------------------

CHAPTER # 3:

Some algos are sensitive to order of training instances, they perform poorly if they get many similar instances in a row
performance evaluation of classifiers is more difficult than of regressor

clone - copies model

StratifiedKFold: performs stratified sampling for classification

cross_val_predict returns predictions which are used in confusion matrix along with labels

Accuracy is not the prefered performance metric for classifiers; especially for skewed datasets

skewed datasets - when some classes occur more frequently than others

SGDClassifier has a decision function, if the score is greater than threshold, it is assigns the instance to positive class, or else it assigns it to negative class
Sklearn doesn't allow to set the threshold directly

cross_val_predict: returns clean prediction for each instance in the training set; clean means that prediction made by the model never saw the data during training

Performance Evaluation:
1) Confusion Matrix used for binary classifers
2) ROC Curve used for binary classifers


For high Precision false positive should be low
For high Recall false negative should be low
Incresing threshold: causes Precision ▲ and recall ▼ and vice versa

F1 score is used to compare 2 classifiers, it's the harmonic mean of precision and recall

ROC curve plots TP rate against FP rate
Area Under Curve(AUC) is used to compare classifiers; 1 being perfect, 0.5 for random classifiers; use roc_auc_score

Prefer Precision-Recall curve when:
1) Positive class is rare
2) We care about FP more than FN
and ROC otherwise

Multiclass Classification:
Can distinguish between more than 2 classes

For binary classifiers OvA strategy is used except for SVM for which OvO strategy is used.

In Confusion Matrix plot:-  
- If col is bright that means many are misclassified in the class, HIGH false positive  
- If row is bright; means that many in the actual class are not properly classified, HIGH false negative

To solve the problem of misclassification:-
- We can add numbers similar to 8 like 3, 5 so that classifier can learn to distinguish them from real 8
- we could feature engineer new features that would help the classifier like count the closed loops 8 has 2, 5 has none, 6 has one-hot
- we could preprocess image

KNN is a multilabel classifier



Titanic:-

for missing Cat attributes, impute it with most repeated one


SVM:-
gamma is a parameter for non-linear hyperparameters. the higher the value the more it tries to fit the training dataset
C controls the tradeoff between smooth decision boundary and classifying the point correctly. Increasing it tends to overfitting.
degree is the parameter when kernel is set to poly. It is basically the degree of polynomial used

NEW GOOD LIB for Data Visualisation:
conda install pandas_profiling. See Chp#3-Exercise-B.ipynb

-----------------------------------------------*************************************************************************************---------------------------------------------

CHP#4:

The more the number of parameters, the more prone to overfitting

Normal Equation:- 
Formula: inv(Xt . X) . Xt . y 
Computational Complexity: O(n^3)

The SVD approach used by sklearn's LinearRegression class is about O(n^2)

Gradient Descent is guranteed to approach arbitarily close to global min if we wait long enough and learning rate is not too high

When using Gradient Descent make sure that all the features have similar scale(use sklearn's standardScaler) else it will take longer to converge

We can use GridSearch to find good learning rate

Main Problem with BGD is that it uses the entire dataset to compute the gradients at every step, which makes it slow.

Stochastic Gradient Descent picks a random instance in the training set and computes the gradients based only on that single instance

When the cost function is very irregular, SGD has a better chance of finding the global minimum than Batch Gradient Descent


If your model is underfitting the training data, adding more training examples will not help. You need to use a more complex model or come up with better features.
Overfitting model can be improved by providing it with more training data until val error reaches the training error


Learning Curves: 
plots of model performance on training set and validation set as a function of training set size,
to generate these, simply train the model several times on different sized subsets of training set

Regularization:-
Regularization significantly reduces the variance of the model, without substantial increase in bias
Till a point, the increase in alpha is beneficial as it is only reducing the variance(hence avoiding overfitting), without loosing any important properties in the data

Bias/variance tradeoff:
Model's Generalization error can be expressed as the sum of three very different errors
Bias: Part of the generalziation error due to wrong assumption; occurs when model underfits
Variance: Part of the generalziation that occurs due to model's excessive sensitivity to small variations in the training data
Irreducible error: Part due to the noisiness of the data itself. Only way to reduce is to clean up the data.

Increasing a model's complexity will typically increase its variance and reduce its bias.


Polynomial models are regularized by reducing the number of polynomial degrees

Scaling is necessary before regularization

Regularized Linear Model:
Done for reducing overfitting
For polynomial model, reduce the number of polynomial degrees
For linear model, regularization is achieved by constraining the weights of the model

1) Ridge Regression:
a regularization term is added to the cost function
ensures that model weights are as small as possible
hyperparameter alpha controls how much you want to regularize the model
Increasing alpha leads to flatter predictions, reducing model's variance but increasing bias

2) Lasso Regression:
it completely eliminates the weights of least important features; very close to 0
It also performs variable selection and yield sparse models

3) Elastic Net:
middle ground between Ridge and Lasso
Used when dataset is large

Difference between Ridge and Lasso:
Ridge does not forces the theta parameter to be 0; this means that irrelevant parameters would be minimized but not eliminated
Lasso overcomes this by actually setting the irrelevant attributes parameters to be 0

Early Stopping:
Another way to regularize iterative learning algorithms suxh as gradient descent is to stop training as soon as validation error reaches minimum
"beautiful free lunch"


Logistic Regression:
Regression for classification

Softmax Regression / multinomial logistic regression: 
Regression for multiclass classification
See formula

-----------------------------------------------*************************************************************************************---------------------------------------------

Chp#5(SVM) Classification:

Capable of performing linear or nonlinear regression, classification and outlier detection
Suitable for complex but small or medium sized datasets

fits the widest possible street between classes. That's why it's called large margin classification

Support vectors: instances at the edge of the cluster and near the decision boundary of SVM

SVMs are sensitive to the feature scales


Hard margin SVM only works when data is:-
1- linearly seperable
2- without error(outliers or noise)

Soft margin classification: find a good balance i.e. keep the street as wide as possible and limit margin violations; in short large margin vs margin violations
this balance is controlled using C hyperparameter in Scikit-Learn
C is inversly propotional to margin length

If overfitting reduce C

SVM classifiers do not output probabilities for each class

Kernel Trick: makes possible to get the same results as if we add many polynomial features without actually adding them [google search]

coef0: controls how much model is concerned with high-degree polynomials vs low-degree polynomials

RBF Kernel:
Increasing gamma makes bell-shape curve narrower, decision boundary becomes more irregular 

γ acts like a regularization hyperparameter: if your model is overfitting, you should reduce it, and if it is underfitting,
you should increase it (similar to the C hyperparameter).


As a rule of thumb, always try the linear kernel first, especially when the training set is large or if it has plenty of features
LinearSVC  is much faster than SVC(kernel="linear")
Try Guassian RBF if training set isn't too large
Ohter kernels exist but are used rarely.
String kernel is used for document classification or DNA analysis

SVM Regression:
goal is to fit as many instances as possible on the street with minimum margin violations
ϵ - controls the width of street


Training a linear SVM classifier means finding the value of w and b that
make this margin as wide as possible while avoiding margin violations (hard margin)
or limiting them (soft margin)

online learning means learning incrementally, typically as new instances arrive

-----------------------------------------------*************************************************************************************---------------------------------------------

Chp#6:

Decision trees are used for regression and classification

They don't require feature scaling at all

CART produces only binary trees, ID3 produce Decision Trees that have more than two children
CART splits the training set into 2 subsets using a single feature k and a threshold tk
CART is a greedy algorithm

Gini vs Entorpy
Most of the time it doesn't make a big difference
Gini is slightly faster so a good default
Entropy produces slightly more balanced trees

Nonparametric model: it surely does has parameters, but number of parameters are not determined prior to training

Increasing min_* hyperparameters or reducing max_* hyperparameters will regularize the model.

See Random Forest(last ques in Ex)

-----------------------------------------------*************************************************************************************---------------------------------------------

Chp#7:

Ensemble - Group of Predictors

Hard voting classifier: majority-vote classifier

If each classifier is a weak learner(slightly better than random guessing), then ensemble can still be a strong learner, provided that
1- there are sufficient number of weak learners
2- they are diverse

Ensemble methods work best when the predictors are as independent from one another as possible. One way to get diverse classifiers is to train them using very different algorithms.
This increases the chance that they will make very different types of errors, improving the ensemble’s accuracy.

In hard voting (also known as majority voting), every individual classifier votes for a class, and the majority wins. In statistical terms, the predicted target label of the 
ensemble is the mode of the distribution of individually predicted labels.[Majority voting]

Soft voting: If all the classifiers are able to estimate class probabilities then you can tell Scikit-Learn to predict the class with highest class probability, averaged over all
individual classifiers. It often achieves higher accuracy than hard voting.

In soft voting, every individual classifier provides a probability value that a specific data point belongs to a particular target class. The predictions are weighted by the 
classifier's importance and summed up. Then the target label with the greatest sum of weighted probabilities wins the vote.[Weighted Average]

Another approach is to use same training algorithm for every predictor, but train them on different random subsets:
1- Bagging: when sampling is performed with replacement
2- Pasting: when sampling is performed without replacement

Out-of-Bag Evaluation:
While bagging, some examples are selected several times while others may not be selected at all.
By defualt a BaggingClassifier samples m training instances with replacement(bootstrap=True), where m is the size of training set
63% of training examples are sampled on average for each predictor.
Remanining 37% that are not sampled are called out-of-bag (oob) instances. Note that they are not the same 37% for all predictors

Aggregation Function for:
1- Classification is statistical mode
2- Regression is average
Aggregation reduces both bias and variance
Net result is that it has the same bias as a single predictor but lower variance

The n_jobs parameter tells Scikit-Learn the number of CPU cores to use for training and predictions
(–1 tells Scikit-Learn to use all available cores)

Random Patches and Random Subspaces:
The BaggingClassifier supports sampling the features as well.
This is controlled by two hyperparameters: max_features and bootstrap_features

Random Forest:
When you are growing a random forest, at each node only a random subset of the features is considered for splitting
Can also tell feature importance

Boosting:
train predictors sequentially, each trying to correct its predecessor

If your AdaBoost ensemble is overfitting the training set, you can try reducing the number of estimators or more strongly regularizing the base estimator

Gradient Boosting:
Instead of tweaking instance weights at every iteration like AdaBoost does, this method tries to fit the new predictor to the residual errors made by previous predictors

Scikit-Learn's GradientBoostingRegressor class is used to train GBRT ensembles; it has hyperparameter:
max_depth, min_sample_leaf control the growth of decision trees, 
n_estimators control the ensemble training

Shrinkage is a regularization technique in which learning_rate hyperparameter scales the contribution of each tree. If it is low, such as 0.1, we will need more trees in the 
ensembe to fit the training set, but the predictions will generalize better.

XGBoost:
Similar to Scikit-Learn in terms of functionality. Used to perform Gradient Boosting.

-----------------------------------------------*************************************************************************************---------------------------------------------

Chp#8:

Curse of Dimensionality: many ml problems have thousands or even millions of features. This make training extremly slow and finding solution difficult

Training speed is surely increased but at the cost of some information loss
It's also extremly useful for DataViz
It all depends on data; sometimes it does not provide better solution
High dimensional datasets are at risk of being very sparse: most training instances are likely to be far away from each other

The more dimensions the training set has, the greater the risk of overfitting

curse of dimensionality can be avoided by increasing the training examples, however, it is impossible bcs for that you would need more training instances than atoms in observable universe

See book for projection and Manifold Learning

Projection: 
All the training instances are projected onto a lower-dimensional subspace
Note always good approach as in many cases subspace may twist and turn as in Swiss roll

Manifold Learning:
generally, a d-dimensional manifold is a part of an n-dimensional space (where d < n) that locally resembles a d-dimensional hyperplane. In the case of the Swiss roll, 
d = 2 and n = 3: it locally resembles a 2D plane, but it is rolled in the third dimension.
Manifold Learning: Many dimensionality reduction algorithms work by modelling the manifold on which training instances lie

PCA:
Unsupervised
First it identifies the hyperplane(axis) that lies closest to the data, then projects the data onto it
Selects the axis that preserves the maximum amount of variance, as it will most likely lose less information than other projections
It assumes that data is centred around the origin. In Scikit-Learn it is automatically done.

Explained Variance Ratio: indicates the propotion of dataset's variance that lies alongside the axis of each principal component
It can be said that the more the variance that lies alongside the axis, the more the information it contains
For example, if the first principal component contains 72% and the second principal component contains 23% then two components together contain 95% of the information

Choosing the right number of dimensions(n_components):
Dimensions are not chosen arbitarily.
Preferably choose dimensions that contain approx 95% of variance.
for dataviz reduce down to 2 or 3

We can get the initial dataset with orignal dimensions by inverse transformation of PCA projection

The mean squared distance between the orignal data and reconstructed data(compressed and then decompressed) is called reconstruction error

Incremental PCA:
Preeceding implementations of PCA require whole training data to fit in the memory so that algorithm can run.
IPCA allows to split the training data into mini batches so that IPCA can be fed with one mini-batch at a time.
Used for large training sets

Randomized PCA:
quickly finds an approximation of the first d components.
Scikit-Learn automatically uses randomized PCA when m or n is greater than 500 and d is less than 80% of m or n, or else it uses the full SVD approach

Kernel PCA:
Useful for nonlinear datasets
Allows to perform complex nonlinear projections for dimensionality reduction.
Good at preserving clusters of instances after projection, or even sometimes unrolling datasets that lie close to twisted manifold.

LLE:
Locally Linear Embedding: powerful nonlinear dimensionality reduction technique(NLDR)
It is a Manifold Learning Technique; doesn't rely on projections
It works by measuring how each instance linearly relates to its closest neighbours, and then looking for a low-dimension representation of the training sets where these locations 
are best preserved
It is good when there is not too much noise

Suppose you perform PCA on a 1,000-dimensional dataset, setting the explained variance ratio to 95%. How many dimensions will the resulting dataset have?
Ans: Depends upon the dataset. It points are perfectly alligned, PCA will reduce the dataset down to 1 dimension and preserve 95% of the variance.
If the points are totally random, then we could have 1 to 1000 dimensions in order to preserve 95% of variance.

Evaluate the performance of dimensionality reduction algo:_
1) perform reverse transformation and calculate the reconstruction error
2) when it cannot be reversed, then use dimensionality reduction as pre-processsing and measure the performance of second algorithm